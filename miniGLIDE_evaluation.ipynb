{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will run a variety of different models on the MSCOCO dataset and test for FID score and Inception score. \n",
    "\n",
    "The models evaluated will be miniGlide-sbucaptions, miniGLIDE-dogs, and miniGLIDE-sbucaptions-simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-fid in c:\\users\\aaron\\anaconda3\\envs\\deeplearningcuda11-openai\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\aaron\\anaconda3\\envs\\deeplearningcuda11-openai\\lib\\site-packages (from pytorch-fid) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\aaron\\anaconda3\\envs\\deeplearningcuda11-openai\\lib\\site-packages (from pytorch-fid) (1.7.3)\n",
      "Requirement already satisfied: torchvision>=0.2.2 in c:\\users\\aaron\\anaconda3\\envs\\deeplearningcuda11-openai\\lib\\site-packages (from pytorch-fid) (0.13.1a0)\n",
      "Requirement already satisfied: torch>=1.0.1 in c:\\users\\aaron\\anaconda3\\envs\\deeplearningcuda11-openai\\lib\\site-packages (from pytorch-fid) (1.13.0.dev20220915)\n",
      "Requirement already satisfied: pillow in c:\\users\\aaron\\anaconda3\\envs\\deeplearningcuda11-openai\\lib\\site-packages (from pytorch-fid) (8.4.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\aaron\\anaconda3\\envs\\deeplearningcuda11-openai\\lib\\site-packages (from torch>=1.0.1->pytorch-fid) (3.10.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\aaron\\anaconda3\\envs\\deeplearningcuda11-openai\\lib\\site-packages (from torchvision>=0.2.2->pytorch-fid) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aaron\\anaconda3\\envs\\deeplearningcuda11-openai\\lib\\site-packages (from requests->torchvision>=0.2.2->pytorch-fid) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aaron\\anaconda3\\envs\\deeplearningcuda11-openai\\lib\\site-packages (from requests->torchvision>=0.2.2->pytorch-fid) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\aaron\\anaconda3\\envs\\deeplearningcuda11-openai\\lib\\site-packages (from requests->torchvision>=0.2.2->pytorch-fid) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aaron\\anaconda3\\envs\\deeplearningcuda11-openai\\lib\\site-packages (from requests->torchvision>=0.2.2->pytorch-fid) (1.26.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-fid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the mscoco dataset and save the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aaron\\Anaconda3\\envs\\deepLearningCuda11-OpenAI\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch as th\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageEnhance\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from glide_text2im.clip.model_creation import create_clip_model\n",
    "from glide_text2im.download import load_checkpoint\n",
    "from glide_text2im.model_creation import (\n",
    "    create_model_and_diffusion,\n",
    "    model_and_diffusion_defaults,\n",
    "    model_and_diffusion_defaults_upsampler,\n",
    ")\n",
    "from glide_text2im.tokenizer.simple_tokenizer import SimpleTokenizer\n",
    "\n",
    "from diffusionHelp import *\n",
    "from modelParameters import *\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "[{'image_id': 139, 'id': 372891, 'caption': 'A woman stands in the dining area at the table.'}, {'image_id': 139, 'id': 376968, 'caption': 'A room with chairs, a table, and a woman in it.'}, {'image_id': 139, 'id': 379917, 'caption': 'A woman standing in a kitchen by a window'}, {'image_id': 139, 'id': 382074, 'caption': 'A person standing at a table in a room.'}, {'image_id': 139, 'id': 384831, 'caption': 'A living area with a television and a table'}]\n"
     ]
    }
   ],
   "source": [
    "annFile = 'D:/Rutgers/Grad Courses/Natural Language Processing/Final Project - MiniGLIDE/GLIDE Local Recreation/GLIDE-Recreation/Evaluation/MSCOCO/annotations/captions_val2017.json'\n",
    "#Load the coco object\n",
    "coco = COCO(annFile)\n",
    "annIds = coco.getAnnIds([139])\n",
    "anns = coco.loadAnns(annIds)\n",
    "print(anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a pytorch dataset class for the MSCOCO dataset, we should return the caption based on the image id\n",
    "\n",
    "class MSCOCODataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, transform=None):\n",
    "        self.transform = transform\n",
    "        # Of the format {image_id: [image_path, caption]}\n",
    "        self.image_ids_to_path = {}\n",
    "        self.image_ids = []\n",
    "\n",
    "        coco = COCO(annotation_file)\n",
    "\n",
    "        for file in os.listdir(image_dir):\n",
    "            if file.endswith('.jpg'):\n",
    "                image_id = int(file.split('.')[0])\n",
    "                self.image_ids.append(image_id)\n",
    "                # Load the annotations and use the first given caption\n",
    "                annIds = coco.getAnnIds([image_id])\n",
    "                anns = coco.loadAnns(annIds)\n",
    "                ann = anns[0]['caption']\n",
    "                self.image_ids_to_path[image_id] = [os.path.join(image_dir, file), ann]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "        _, caption = self.image_ids_to_path[image_id]\n",
    "\n",
    "        return image_id, caption\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def show_images(batch: th.Tensor, brightness: float = 1.0):\n",
    "        \"\"\" Display a batch of images inline with adjustable brightness. \"\"\"\n",
    "        \n",
    "        # Check if the batch has 3 or 4 dimensions\n",
    "        if batch.ndim == 3:\n",
    "            batch = batch.unsqueeze(0)\n",
    "        \n",
    "        # Apply brightness adjustment\n",
    "        batch = batch * brightness\n",
    "\n",
    "        # Ensure the values are within the valid range for image display\n",
    "        scaled = ((batch)*(127.5)).round().clamp(0,255).to(th.uint8).cpu()\n",
    "\n",
    "        # Rearrange dimensions for image display\n",
    "        reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])\n",
    "\n",
    "        # Display the image\n",
    "        display(Image.fromarray(reshaped.numpy()))\n",
    "        \n",
    "    def saveImages(self, dir, image_ids, images, brightness_factor=1.8):\n",
    "        def pad_with_zeros(value, total_length):\n",
    "            return \"{:0>{}}\".format(value, total_length)\n",
    "        \n",
    "        images = images * brightness_factor\n",
    "        scaled = ((images)*(127.5)).round().clamp(0,255).to(th.uint8).cpu()\n",
    "\n",
    "        # Iterate through each image and its corresponding ID in the batch\n",
    "        for image_id, image in zip(image_ids, scaled):\n",
    "            # Convert the PyTorch tensor to a PIL image\n",
    "            image_pil = to_pil_image(image.cpu())\n",
    "\n",
    "            # Save the image\n",
    "            out = pad_with_zeros(image_id.item(), 12)\n",
    "            image_pil.save(os.path.join(dir, str(out) + '.jpg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['D:/Rutgers/Grad Courses/Natural Language Processing/Final Project - MiniGLIDE/GLIDE Local Recreation/GLIDE-Recreation/Evaluation/Evaluation Models/dhariwal_sbucaptions_100_epoch18_ILAB.pt',\n",
    "          'D:/Rutgers/Grad Courses/Natural Language Processing/Final Project - MiniGLIDE/GLIDE Local Recreation/GLIDE-Recreation/Evaluation/Evaluation Models/model_Lsimple_sbucaptions_v3.pt',\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "has_cuda = th.cuda.is_available()\n",
    "device = th.device('cpu' if not has_cuda else 'cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "time_steps = 100\n",
    "guidance_scale = 3 \n",
    "mscoco = MSCOCODataset('D:/Rutgers/Grad Courses/Natural Language Processing/Final Project - MiniGLIDE/GLIDE Local Recreation/GLIDE-Recreation/Evaluation/MSCOCO/images/val2017', annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total base parameters 156947494\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "\n",
    "options = model_and_diffusion_defaults()\n",
    "options['use_fp16'] = False\n",
    "options['diffusion_steps'] = time_steps # use 100 diffusion steps for fast sampling\n",
    "options['num_channels'] = 96\n",
    "options['num_head_channels'] = 32\n",
    "options['num_res_blocks'] = 3\n",
    "options['xf_width'] = 512\n",
    "options['xf_layers'] = 16\n",
    "options['xf_heads'] = 8\n",
    "model, diffusion = create_model_and_diffusion(**options)\n",
    "model.load_state_dict(th.load(models[1]))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print('total base parameters', sum(x.numel() for x in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total upsampler parameters 398361286\n"
     ]
    }
   ],
   "source": [
    "# Load upsampling model\n",
    "options_up = model_and_diffusion_defaults_upsampler()\n",
    "options_up['use_fp16'] = has_cuda\n",
    "options_up['timestep_respacing'] = 'fast27' # use 27 diffusion steps for very fast sampling\n",
    "model_up, diffusion_up = create_model_and_diffusion(**options_up)\n",
    "model_up.eval()\n",
    "if has_cuda:\n",
    "    model_up.convert_to_fp16()\n",
    "model_up.to(device)\n",
    "model_up.load_state_dict(th.load('D:/Rutgers/Grad Courses/Natural Language Processing/Final Project - MiniGLIDE/Git/MiniGLIDE/glide_model_cache/upsample.pt'))\n",
    "print('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model function for classifier free sampling\n",
    "def model_fn(x_t, ts, guidance_scale=guidance_scale, **kwargs):\n",
    "        half = x_t[: len(x_t) // 2]\n",
    "        combined = th.cat([half, half], dim=0)\n",
    "        model_out = model(combined, ts, **kwargs)\n",
    "        eps, rest = model_out[:, :3], model_out[:, 3:]\n",
    "        cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim=0)\n",
    "        half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n",
    "        eps = th.cat([half_eps, half_eps], dim=0)\n",
    "        return th.cat([eps, rest], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 312/313 [6:54:06<01:19, 79.64s/it]  \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (16) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9436/4273630711.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Sample with classifier Free Guidance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0msamples_CF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturnSample_CF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiffusion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mmodel_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model_kwargs_upsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples_CF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mup_samples_CF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturnUpSample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_up\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiffusion_up\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions_up\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupsample_temp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcond_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Rutgers\\Grad Courses\\Natural Language Processing\\Final Project - MiniGLIDE\\GLIDE Local Recreation\\GLIDE-Recreation\\diffusionHelp.py\u001b[0m in \u001b[0;36mreturnSample_CF\u001b[1;34m(model, model_fn, model_kwargs, device, batch_size, options, diffusion, clip_denoised, noise, progress, cond_fn)\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mcond_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcond_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     )\n\u001b[0;32m    125\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdel_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Aaron\\Anaconda3\\envs\\deepLearningCuda11-OpenAI\\lib\\site-packages\\glide_text2im\\gaussian_diffusion.py\u001b[0m in \u001b[0;36mp_sample_loop\u001b[1;34m(self, model, shape, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress)\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m             \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m             \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m         ):\n\u001b[0;32m    400\u001b[0m             \u001b[0mfinal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Aaron\\Anaconda3\\envs\\deepLearningCuda11-OpenAI\\lib\\site-packages\\glide_text2im\\gaussian_diffusion.py\u001b[0m in \u001b[0;36mp_sample_loop_progressive\u001b[1;34m(self, model, shape, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress)\u001b[0m\n\u001b[0;32m    446\u001b[0m                     \u001b[0mdenoised_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdenoised_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[0mcond_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcond_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m                     \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m                 )\n\u001b[0;32m    450\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Aaron\\Anaconda3\\envs\\deepLearningCuda11-OpenAI\\lib\\site-packages\\glide_text2im\\gaussian_diffusion.py\u001b[0m in \u001b[0;36mp_sample\u001b[1;34m(self, model, x, t, clip_denoised, denoised_fn, cond_fn, model_kwargs)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0mclip_denoised\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclip_denoised\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m             \u001b[0mdenoised_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdenoised_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         )\n\u001b[0;32m    348\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Aaron\\Anaconda3\\envs\\deepLearningCuda11-OpenAI\\lib\\site-packages\\glide_text2im\\respace.py\u001b[0m in \u001b[0;36mp_mean_variance\u001b[1;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mp_mean_variance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_mean_variance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcondition_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcond_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Aaron\\Anaconda3\\envs\\deepLearningCuda11-OpenAI\\lib\\site-packages\\glide_text2im\\gaussian_diffusion.py\u001b[0m in \u001b[0;36mp_mean_variance\u001b[1;34m(self, model, x, t, clip_denoised, denoised_fn, model_kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[0mmodel_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mmodel_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Aaron\\Anaconda3\\envs\\deepLearningCuda11-OpenAI\\lib\\site-packages\\glide_text2im\\respace.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, ts, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mmap_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestep_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mnew_ts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_ts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9436/684820355.py\u001b[0m in \u001b[0;36mmodel_fn\u001b[1;34m(x_t, ts, guidance_scale, **kwargs)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mhalf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mcombined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhalf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhalf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mmodel_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mcond_eps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muncond_eps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Aaron\\Anaconda3\\envs\\deepLearningCuda11-OpenAI\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Aaron\\Anaconda3\\envs\\deepLearningCuda11-OpenAI\\lib\\site-packages\\glide_text2im\\text2im_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, timesteps, tokens, mask)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mtext_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text_emb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0mxf_proj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxf_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"xf_proj\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"xf_out\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mxf_proj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mxf_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (16) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "save_dir = 'D:/Rutgers/Grad Courses/Natural Language Processing/Final Project - MiniGLIDE/GLIDE Local Recreation/GLIDE-Recreation/Evaluation/MSCOCO/images/model_out/miniGLIDEsimple'\n",
    "\n",
    "dataloader = DataLoader(mscoco, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for batch_idx, (image_ids, captions) in enumerate(tqdm(dataloader)):\n",
    "    # print(image_ids)\n",
    "    # print(captions)\n",
    "    prompts = list(captions)\n",
    "\n",
    "    # Create the text tokens to feed to the model.\n",
    "    model_kwargs = get_model_kwargs_classifier_free(prompts, model, options, device)\n",
    "\n",
    "    # Sample with classifier Free Guidance\n",
    "    samples_CF = returnSample_CF(model, model_fn, model_kwargs, device, batch_size, options, diffusion, progress=False)[:batch_size]\n",
    "    model_kwargs = get_model_kwargs_upsample(prompts, samples_CF, model, options, device)\n",
    "    up_samples_CF = returnUpSample(model_up, diffusion_up, batch_size, device, model_kwargs, options_up, upsample_temp=1.0, cond_fn=None, progress=False)\n",
    "    mscoco.saveImages(save_dir, image_ids, up_samples_CF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pytorch_fid import fid_score\n",
    "from PIL import Image\n",
    "\n",
    "def resize_images(input_dir, output_dir, size=(256, 256)):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        img_path = os.path.join(input_dir, img_name)\n",
    "        img = Image.open(img_path)\n",
    "        img_resized = img.resize(size)\n",
    "        img_resized.save(os.path.join(output_dir, img_name))\n",
    "\n",
    "coco_dir = 'D:/Rutgers/Grad Courses/Natural Language Processing/Final Project - MiniGLIDE/GLIDE Local Recreation/GLIDE-Recreation/Evaluation/MSCOCO/images/val2017'\n",
    "resized_coco_dir = 'D:/Rutgers/Grad Courses/Natural Language Processing/Final Project - MiniGLIDE/GLIDE Local Recreation/GLIDE-Recreation/Evaluation/MSCOCO/images/val2017_256'\n",
    "\n",
    "# Resize the real images to the same size as generated images (256x256)\n",
    "resize_images(coco_dir, resized_coco_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid(directory1, directory2):\n",
    "    device = 'cuda' if th.cuda.is_available() else 'cpu'\n",
    "    fid_value = fid_score.calculate_fid_given_paths([directory1, directory2], 50, device, 2048)\n",
    "    return fid_value\n",
    "\n",
    "\n",
    "mini_GLIDE_simple_dir = 'D:/Rutgers/Grad Courses/Natural Language Processing/Final Project - MiniGLIDE/GLIDE Local Recreation/GLIDE-Recreation/Evaluation/MSCOCO/images/model_out/miniGLIDEsimple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to C:\\Users\\Aaron/.cache\\torch\\hub\\checkpoints\\pt_inception-2015-12-05-6726825d.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f89a5458d64f2d9ea170e45b57d4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/91.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:27<00:00,  3.64it/s]\n",
      "100%|██████████| 100/100 [00:27<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID score: 202.16732688719262\n"
     ]
    }
   ],
   "source": [
    "# Let's first calculate the fid score for miniGLIDEsimple\n",
    "fid_value = calculate_fid(mini_GLIDE_simple_dir, resized_coco_dir)\n",
    "print(f\"FID score: {fid_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.49it/s]\n",
      "100%|██████████| 100/100 [00:13<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID score: 145.76802162377993\n"
     ]
    }
   ],
   "source": [
    "mini_GLIDE_dir = 'D:/Rutgers/Grad Courses/Natural Language Processing/Final Project - MiniGLIDE/GLIDE Local Recreation/GLIDE-Recreation/Evaluation/MSCOCO/images/model_out/iLab_epoch18'\n",
    "\n",
    "# Now calculate the fid score for miniGLIDE\n",
    "fid_value = calculate_fid(mini_GLIDE_dir, resized_coco_dir)\n",
    "print(f\"FID score: {fid_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearningCuda11-OpenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
